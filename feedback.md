# Assignment Feedback: Week 04 Dimensionality Reduction

**Student:** rafeburns
**Raw Score:** 41/46 (89.1%)
**Course Points Earned:** 68.0

---

## Problem Breakdown

### Exercise 2 (6/6 = 100.0%)

**Part ex1-part2** (ex1-part2.code): 3/3 points

_Feedback:_ Good job: you trained a KNN on the t-SNE features, did a proper train/test split, and reported accuracy. This meets the task’s goal. For robustness, you could also try varying K or use cross-validation, but your approach is correct.

**Part ex1-part3** (ex1-part3.code): 3/3 points

_Feedback:_ Correct: you used UMAP features with KNN, did a proper train/test split, fit, predicted, and computed accuracy. Variable usage is consistent and aligns with the UMAP-based objective. Nice work.

---

### Exercise 4 (15/20 = 75.0%)

**Part ex2-part1** (ex2-part1.code): 3/7 points

_Feedback:_ Good attempt applying PCA with KNN and exploring 1–3 components. However, you incorrectly fit PCA on X_test (data leakage). Fit on X_train and use pca.transform(X_test). This invalidates reported accuracies. Optionally plot first 2 PCs for inspection or use variance-based PCA.

**Part ex2-part2** (ex2-part2.code): 7/7 points

_Feedback:_ Well done. You correctly applied UMAP (fit on train, transform on test) and evaluated KNN across 1–3 components. This meets the task goals and is methodologically sound. Consider tuning UMAP/knn hyperparameters or adding a 2D plot for insight.

**Part ex2-part3** (ex2-part3.answer): 5/6 points

_Feedback:_ You compared PCA vs UMAP and justified why PCA might win given linear separation—good. You explored components (1–3) for both methods. However, you didn’t discuss UMAP’s typical advantage in low dimensions with low n_neighbors, which was expected.

---

### Exercise 1 (20/20 = 100.0%)

**Part pipeline-part1** (pipeline-part1.code): 4/4 points

_Feedback:_ Good job: you reduced to 2D with PCA and plotted a scatter colored by class, meeting the goal. For clarity, consider adding a colormap (e.g., cmap='jet'), a colorbar, title/axis labels, and point size, but these aren’t required.

**Part pipeline-part2** (pipeline-part2.code): 4/4 points

_Feedback:_ Good scree plot logic: fits full PCA, takes first 40 components, uses percent variance, and plots vs component index. Consider adding axis labels/title and plt.show() for clarity, but the core requirement is met.

**Part pipeline-part3** (pipeline-part3.code): 4/4 points

_Feedback:_ Correct approach. You use the fitted pca_high_dim from prior work, compute the cumulative explained variance, and find the minimal components to reach 95%. This aligns with the task and your pipeline. Well done.

**Part pipeline-part4** (pipeline-part4.code): 4/4 points

_Feedback:_ Great job. You used the n_components from Step 4, fit PCA on the training set, reduced the chosen digit, reconstructed it, and plotted the result. Approach is correct and meets the objective. Minor note: calling fit_transform then transform is redundant, but harmless.

**Part pipeline-part5** (pipeline-part5.code): 4/4 points

_Feedback:_ Good job: you compare KNN with and without PCA and aim to preserve 80% variance. Minor issue: n_components is chosen from pca_high_dim fit on different data; better compute it on MNIST or use PCA(n_components=0.8) directly. Otherwise technique and implementation are solid.

---

## Additional Information

This feedback was automatically generated by the autograder.

**Generated:** 2025-10-28 19:52:04 UTC

If you have questions about your grade, please reach out to the instructor.